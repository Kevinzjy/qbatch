#!/usr/bin/env python3
# TODO: add support for multiple pbs_server
import re
import os
import sys
import json
import copy
import shutil
from time import sleep
from pathlib import Path
from datetime import datetime
from subprocess import getstatusoutput
from collections import namedtuple, defaultdict
__version__ = '0.1.0'
PATTERN = re.compile(r'\{.*?\}')
HIGHLIGHT = "\033[1;32m"
NORMAL = "\033[0m"

# Check python version
if not sys.version.startswith('3'):
    sys.stderr.write(f"Error traceback:\n  python version {sys.version}\n")
    sys.stderr.write(f"\nFailed reason:\n  only {HIGHLIGHT}python3.6+{NORMAL} is supported!\n")
    sys.exit(1)

# Import tomli libarary
try:
    import tomli
except ModuleNotFoundError as e:
    sys.stderr.write(f"Error traceback:\n  import tomli\n")
    sys.stderr.write(f"\nFailed reason:\n  the {HIGHLIGHT}tomli'{NORMAL} library is not installed, run {HIGHLIGHT} 'pip install tomli'{NORMAL} to install it.\n")
    sys.exit(1)


def ensure_file_exist(file_name):
    """Make sure file exists

    Args:
        file_name (str): path to file to check

    Returns:
        Path: pathlib Path of given file
    """
    file_path = Path(file_name)
    if not file_path.exists():
        warn(f"File not exists: {file_name}")
        exit()

    return file_path


def timestamp():
    """Get current timestamp

    Returns:
        str: formatted time stamp
    """
    now = datetime.now()
    t = now.strftime("%a %Y-%m-%d %H:%M:%S")
    return t


def strptime(s):
    t = datetime.strptime(s, "%a %Y-%m-%d %H:%M:%S")
    return t.strftime("%Y%m%d%H%M%S")


class Pipeline(object):

    def __init__(self, file_name):
        """Load TOML configuration into Pipeline object

        Args:
            file_name (str): path to TOML file
        """
        # Load TOML configuration
        toml_file = ensure_file_exist(file_name)
        with open(toml_file, "rb") as f:
            self.dict = tomli.load(f)

        # Naive check of configuration file
        for key in ['pipeline', 'version', 'work_dir', 'sample_sheet', 'job']:
            if key not in self.dict:
                sys.stderr.write(f"\nError traceback:\n{key} not found in {file_name}\n")
                sys.stderr.write(f"\nFailed reason:\nrequired field {HIGHLIGHT}{key}{NORMAL} not found in the header of toml file.\n")
                sys.exit(1)

        # Create work_dir
        self.work_dir = Path(self.dict['work_dir'])
        if not self.work_dir.exists():
            self.work_dir.mkdir()

        # Directory to store scripts
        self.script_dir = self.work_dir / "sc"
        if not self.script_dir.exists():
            self.script_dir.mkdir()

        # Make a copy of configuration file
        self.json = self.work_dir / "info"
        self.lock = self.work_dir / "jobs"
        self.checkpint = self.work_dir / "checkpoint"
        self.jobs = None
        self.toml = self.work_dir / "pipeline"
        if not self.toml.exists():
            shutil.copy(file_name, str(self.toml))

        # Directory to store log files
        self.log_dir = self.work_dir / "logs"
        if not self.log_dir.exists():
            self.log_dir.mkdir()


    def __getitem__(self, key):
        """Expose self.dict as iterator values"""
        keys = key.split('.')
        p = self.dict
        for i in keys:
            if i not in p:
                raise KeyError(f"Value {key} not found.")
            p = p[i]
        return p

    def __iter__(self):
        """Expose self.dict as iterator"""
        return iter(self.dict)

    def init(self):
        """Init pipeline object"""
        t = self.load_json()
        self.jobs = self.load_checkpoint()

        # Move old files
        if t is not None:
            if self.json.exists():
                self.json.rename(self.log_dir / f"info.{t}")
            if self.checkpint.exists():
                self.checkpint.rename(self.log_dir / f"checkpoint.{t}")
            if self.lock.exists():
                self.lock.rename(self.log_dir / f"jobs.{t}")

        # Record some information
        header = {
            "Python version": sys.version.split(' ')[0],
            "Pysched version": __version__,
            "Command line": ' '.join(sys.argv),
            "Pipeline name": self['pipeline'],
            "Pipeline version": self['version'],
            "Submit time": timestamp(),
        }
        with open(self.json, 'w') as js:
            json.dump(header, js)

        print("===========================================")
        k_len = max([len(i) for i in header]) + 1
        for k, v in header.items():
            print(f"{k+':'+(k_len-len(k))*' '}{v}")
        print("===========================================")

        # Load sample sheet
        self.samples = self.load_sample_sheet()

    def load_sample_sheet(self):
        """Load sample sheet

        Returns:
            list: list of sample as namedtuple
        """
        ss_file = self['sample_sheet']
        samples = []
        with open(ss_file, 'r') as f:
            field_names = f.readline().rstrip().split(',')
            Sample = namedtuple("Sample", field_names)
            for line in f:
                content = line.rstrip().split(',')
                samples.append(Sample(*content))

        return samples

    def load_checkpoint(self):
        """Get unfinished jobs in the last run

        Returns:
            list: list of unfinished jobs
        """
        if not self.checkpint.exists() and not self.lock.exists():
            return None

        checkpoint = []
        if self.checkpint.exists():
            with open(self.checkpint, 'r') as f:
                for line in f:
                    checkpoint.append(line.rstrip())

        jobs = []
        if self.lock.exists():
            with open(self.lock, 'r') as f:
                for line in f:
                    job_name, job_id, job_cmd = line.rstrip().split('\t')
                    if job_id in checkpoint:
                        continue
                    jobs.append(job_name)

        return jobs

    def load_json(self):
        """Load last submit time stamp from json file

        Returns:
            str: formatted timestamp
        """
        if self.json.exists():
            with open(self.json, 'r') as f:
                js = json.load(f)
                t = strptime(js['Submit time'])
                return t
        else:
            return None

    def run(self):
        lock = open(self.lock, 'w')
        job_ids = {}

        # For each job
        for job_name, job_dict in self['job'].items():
            template = Template(job_name, job_dict)

            # Submit job for each sample
            for sample in self.samples:
                # Init job object & parse job for each sample
                job = Job(job_name + "." + sample.name, template.dict)
                job.map_sample(sample, self.dict)

                # Create PBS job script
                pbs_file = self.script_dir / (job.name + ".sh")
                pbs_job = PBSJob(job, self.dict, pbs_file)
                pbs_job.write()

                # If checkpoint defined
                if self.jobs is not None:
                    # If job has been finished
                    if job.name not in self.jobs:
                        job_ids[job.name] = None
                        continue

                # Get dependencies
                depend_ids = []
                if 'depend' in job:
                    if isinstance(job['depend'], list):
                        for i in job['depend']:
                            d_id = job_ids[i+'.'+sample.name]
                            depend_ids.append(d_id)
                    else:
                        d_id = job_ids[job['depend']+'.'+sample.name]
                        depend_ids.append(d_id)
                depend_ids = [i for i in depend_ids if i is not None]

                # Submit PBS job
                pbs_cmd, pbs_id = pbs_job.submit(depend_ids)
                job_ids[job.name] = pbs_id

                # Record submission
                if pbs_cmd is not None:
                    lock.write(f"{job.name}\t{pbs_id}\t{pbs_cmd}\n")
        lock.close()

        n_skip = sum([1 for i in job_ids.values() if i is None])
        n_total = len(job_ids)
        print(f"Submitted {n_total-n_skip} jobs, skipped {n_skip} jobs for {len(self.samples)} samples.")


class Template(object):

    def __init__(self, job_name, job_dict):
        """Init job from job dict

        Args:
            job_name (str): name of job
            job_dict (dict): job dict
        """
        self.name = job_name
        template_dict = job_dict.copy()
        self.nested_dict(template_dict)
        self.dict = template_dict
        for x in ['shell']:
            if x not in self.dict:
                sys.stderr.write(f"\nError traceback:\n{HIGHLIGHT}{x}{NORMAL} not found in {HIGHLIGHT}{self.name}{NORMAL}\n")
                sys.stderr.write(f"\nFailed reason:\nthe [shell] fields should be specified for each job.\n")
                sys.exit(1)

    def __getitem__(self, key):
        """Expose self.dict as iterator values"""
        keys = key.split('.')
        p = self.dict
        for i in keys:
            if i not in p:
                raise KeyError(f"Value {key} not found.")
            p = p[i]
        return p

    def __iter__(self):
        """Expose self.dict as iterator"""
        return iter(self.dict)

    def nested_dict(self, d):
        """Convert raw job dict to nested dict
        Due to the limitation of TOML format, nested dict is not elegant and easy to use.
        So use list of single-dict instead. This function is intend to convert list of
        single-dict to nested dicts. For example:

        Raw TOML:
            output = [
                {fq = "{work_dir}/out_guppy/{sample.name}.fastq.gz"},
                {pass_fq = "{work_dir}/out_guppy/{sample.name}.pass.fastq.gz"},
            ]

        Nested dict:
            output = {
                fq: "{work_dir}/out_guppy/{sample.name}.fastq.gz",
                pass_fq: "{work_dir}/out_guppy/{sample.name}.pass.fastq.gz",
            }

        Args:
            d (tree): nested job dict
        """
        for k, v in d.items():
            if isinstance(v, dict):
                self.nested_dict(d[k])
            elif isinstance(v, list):
                if k in ['input', 'output']:
                    new_v = {}
                    for i in v:
                        if not isinstance(i, dict):
                            sys.stderr.write(f"\nError traceback:\ncan not parse {HIGHLIGHT}{i}{NORMAL} in {HIGHLIGHT}{k}{NORMAL}\n")
                            sys.stderr.write(f"\nFailed reason:\nEvery element of a list must be dict.\n")
                            sys.exit(0)
                        new_v.update(i)
                    d[k] = new_v
                    self.nested_dict(d[k])
            else:
                pass


class Job(Template):

    def __init__(self, job_name, job_dict):
        """Init sub job from template name and dict

        Args:
            job_name (str): name of job
            job_dict (dict): nested dict of Template
        """
        self.name = job_name
        self.dict = copy.deepcopy(job_dict)

    def map_sample(self, sample, env):
        """Map template dict using sample and pipeline arguments

        Args:
            sample (Sample): namedtuple of sample
            env (dict): dict of Pipeline object
        """
        self.resolve(self.dict, sample, env)

    def resolve(self, d, sample, env):
        """Iterative through each leaf node to parse parameters using pipeline environment
        and sample attributes

        Args:
            d (dict): dict to parse
            sample (Sample): namedtuple of sample
            env (dict): dict of Pipeline object
        """
        for k, v in d.items():
            if isinstance(v, dict):
                self.resolve(d[k], sample, env)
            elif isinstance(v, str):
                params = self.get_param(v)
                if params is None:
                    pass
                else:
                    formatter = {}
                    for param in params:
                        p_value = None

                        # From sample attributes
                        if param.split('.')[0] == "sample":
                            s_k = param.split('.')[1]
                            if s_k not in sample._fields:
                                sys.stderr.write(f"\nError traceback:\nfield {s_k} not found for sample {sample}\n")
                                sys.stderr.write(f"\nFailed reason:\nthe {HIGHLIGHT}{param}{NORMAL} requires each sample has {HIGHLIGHT}{s_k}{NORMAL} field.\n")
                                sys.exit(0)
                            p_value = getattr(sample, s_k)
                            formatter[param] = p_value
                            continue

                        # From local variables
                        try:
                            p_value = self[param]
                            formatter[param] = p_value
                            continue
                        except KeyError:
                            pass

                        # From global variables
                        try:
                            p_value = env[param]
                            formatter[param] = p_value
                            continue
                        except KeyError:
                            pass

                        # Not found
                        sys.stderr.write(f"\nError traceback:\n{v.strip()}\n")
                        sys.stderr.write(f"\nFailed reason:\nfield {HIGHLIGHT}{param}{NORMAL} not found.\n")
                        sys.exit(0)

                    new_v = v
                    for i, j in formatter.items():
                        new_v = new_v.replace("{"+str(i)+"}", str(j))
                    d[k] = new_v
            else:
                pass

    @staticmethod
    def get_param(cmd):
        """Get parameters inside brace

        Args:
            cmd (str): command lines to parse

        Returns:
            list: list of extracted parameters
        """
        params = re.findall(PATTERN, cmd)
        if len(params) > 0:
            return list(set([i.strip('{}') for i in params]))
        else:
            return None

    def is_finished(self):
        # No defined output file
        if 'output' not in self.dict:
            return False

        # Determine if output has been generated
        flag = True
        # For multiple output files
        if isinstance(self['output'], dict):
            for k, v in self['output'].items():
                p = Path(v)
                if not p.exists():
                    flag = False
        else:
            # For single output file
            p = Path(self['output'])
            if not p.exists():
                flag = False
        return flag


class PBSJob(object):

    def __init__(self, job, env, pbs_file):
        self.job_name = job.name
        self.skip = job.is_finished()

        # Job scheduler
        self.scheduler = env['pbs_scheduler']
        if self.scheduler not in ['slurm', 'torque']:
            sys.stderr.write(f"\nError traceback:\n{self.scheduler}\n")
            sys.stderr.write(f"\nFailed reason:\nunsupported scheduler, only slurm and torque is supported.\n")
            sys.exit(0)

        # Queue name
        if 'queue' in job:
            self.queue = job['queue']
        elif 'pbs_queue' in env:
            self.queue = env['pbs_queue']
        else:
            sys.stderr.write(f"\nFailed reason: no queue specified, please use 'pbs_queue' in the header section or 'queue' in each job.\n")
            sys.exit(0)

        # Walltime
        if 'walltime' in job:
            self.walltime = job['walltime']
        elif 'pbs_walltime' in env:
            self.walltime = env['pbs_walltime']
        else:
            sys.stderr.write("\nFailed reason: no walltime specified, please use 'pbs_walltime' in the header section or 'walltime' in each job.\n")
            sys.exit(0)

        # Work directory
        if 'work_dir' in job:
            self.work_dir = job['work_dir']
        elif 'work_dir' in env:
            self.work_dir = env['work_dir']
        else:
            sys.stderr.write("\nFailed reason: no work dir specified, please use 'work_dir' in the header section.\n")
            sys.exit(0)

        # Other parameters
        if 'log' in job:
            self.log = job['log']
        else:
            self.log = f"{env['work_dir']}/{self.job_name}.logs"

        if 'threads' in job:
            self.threads = job['threads']
        else:
            self.threads = 1

        if 'gpus' in job:
            self.gpus = job['gpus']
        else:
            self.gpus = 0

        self.shell = job['shell']
        self.pbs_file = str(pbs_file)

    def write(self):
        if self.scheduler == 'slurm':
            self.write_slurm()
        else:
            self.write_torque()

    def write_slurm(self):
        with open(self.pbs_file, 'w') as out:
            out.write(f"#!/bin/bash\n")
            out.write(f"#SBATCH -J {self.job_name}\n")
            out.write(f"#SBATCH -p {self.queue}\n")
            out.write(f"#SBATCH -n {self.threads}\n")
            out.write(f"#SBATCH -o {self.log}\n")
            out.write(f"#SBATCH -e {self.log}\n")
            out.write(f"#SBATCH --mail-type=ALL\n")
            out.write(f"#SBATCH -t {self.walltime}:00:00\n")
            out.write(f"#SBATCH -D {self.work_dir}\n")
            out.write(f"set -e\n")
            out.write(f"cd {self.work_dir}\n")
            out.write(self.shell)
            out.write(f"echo $SLURM_JOB_ID >> {self.work_dir}/checkpoint\n")

    def write_torque(self):
        with open(self.pbs_file, 'w') as out:
            out.write(f"#!/bin/bash\n")
            out.write(f"#PBS -N {self.job_name}\n")
            out.write(f"#PBS -q {self.queue}\n")
            out.write(f"#PBS -o {self.log}\n")
            out.write(f"#PBS -e {self.log}\n")
            if self.gpus != 0:
                out.write(f"#PBS -l nodes=1:ppn={self.threads}:gpus={self.gpus}\n")
            else:
                out.write(f"#PBS -l nodes=1:ppn={self.threads}\n")
            out.write(f"#PBS -l walltime={self.walltime}:00:00\n")
            out.write(f"#PBS -d {self.work_dir}\n")
            out.write(f"set -e\n")
            out.write(f"cd {self.work_dir}\n")
            out.write(self.shell)
            out.write(f"echo $PBS_JOBID >> {self.work_dir}/checkpoint\n")

    def submit(self, depend_ids=[]):
        if self.skip:
            sys.stderr.write(f"Skip {self.job_name} because it has been finished.\n")
            return None, None

        if self.scheduler == 'slurm':
            return self.submit_slurm(depend_ids)
        else:
            return self.submit_torque(depend_ids)

    def submit_slurm(self, depend_ids=[]):
        # Get dependencies
        if len(depend_ids) > 0:
            cmd = f"sbatch --dependency=afterok:{':'.join(depend_ids)} {self.pbs_file}"
        else:
            cmd = f"sbatch {self.pbs_file}"

        status, ret = getstatusoutput(cmd)

        if status != 0:
            sys.stderr.write(f"\nError traceback:\n{cmd}\n{ret}")
            sys.stderr.write(f"\nFailed reason:\nfailed to submit job {self.pbs_file}\n")
            sys.exit(0)
        else:
            sleep(0.1)
            qid = ret.split(' ')[-1]
            return cmd, qid

    def submit_torque(self, depend_ids=[]):
        # Get dependencies
        if len(depend_ids) > 0:
            cmd = f"qsub -W depend=afterok:{':'.join(depend_ids)} {self.pbs_file}"
        else:
            cmd = f"qsub {self.pbs_file}"

        status, ret = getstatusoutput(cmd)
        if status != 0:
            sys.stderr.write(f"\nError traceback:\n{cmd}\n{ret}\n")
            sys.stderr.write(f"\nFailed reason:\nfailed to submit job {self.pbs_file}\n")
            sys.exit(0)
        else:
            sleep(0.1)
            return cmd, ret

def main():
    import argparse

    # Init parser
    parser = argparse.ArgumentParser(
        prog='qbatch', description='a python-based job scheduler',
        epilog='Support native torque/slurm for build analysis pipeline'
    )
    parser.add_argument('toml', metavar='toml',
                        help='path to input toml file')
    parser.add_argument('-v', '--version', action='version',
                        version='%(prog)s v{version}'.format(version=__version__))
    args = parser.parse_args()

    toml_file = args.toml
    pl = Pipeline(toml_file)
    pl.init()
    pl.run()


if __name__ == "__main__":
    main()
